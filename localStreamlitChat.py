# to run execute: > streamlit run localStreamlitChat.py 
# UI framework for humans:
import streamlit as st
# redis imports: for caching prompts and responses and 
# searching using Vector Similarity for previous prompts 
import redis
from redis.commands.search.field import VectorField
from redis.commands.search.query import Query
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
#LLM related imports:
from sentence_transformers import SentenceTransformer
from langchain.llms import GPT4All
from langchain.chains import LLMChain
from langchain.prompts.prompt import PromptTemplate
# general imports
import re,time,sys

### General Setup / functions: ###
## this function is designed to return a likely to be unique value for a string
## a bloom or cuckoo filter used for deduping would be better
## redis has a bloom module designed for that purpose
def compact_string_for_keyname(payload):
    #print(f'rem_vowel: payload is of type {type(payload)}')
    payload_string1 = payload[0]
    payload_string1 = payload_string1.replace(" ", "")
    #print(f'payload_string1 == {payload_string1} and is of type: {type(payload_string1)}')
    sumchars=0
    for s in payload_string1:
        sumchars = sumchars+ord(s)
    response = f'{re.sub("[aeiouAEIOU]","",payload_string1)}:{sumchars}'
    return(response)

### Redis Setup / functions: ###
## checks sys.args for host and port etc...
redis_host = 'redis-12000.homelab.local'
redis_port = 12000
redis_password = ""
redis_user = "default"
if len(sys.argv) > 2:
    redis_host=sys.argv[1]
    redis_port=sys.argv[2]
if len(sys.argv) > 3:
    redis_password=sys.argv[3]
if len(sys.argv) > 4:
    redis_user = sys.argv[5]

if redis_password == "" and redis_user == "default":
    redis_connection = redis.Redis( host=redis_host, port=redis_port, encoding='utf-8', decode_responses=True)
else:
    redis_connection = redis.Redis( host=redis_host, port=redis_port, password=redis_password ,username=redis_user, encoding='utf-8', decode_responses=True)

index_name = 'idx_vss'

# this function executes the VSS search call against Redis
# it accepts a redis index (generated by calling connection)
def vec_search(vindex,query_vector_as_bytes):
    # KNN 10 specifies to return only up to 10 nearest results (could be unrelated)
    # VECTOR_RANGE specifies the prompts must be similar
    query =(
        Query(f'(@embedding:[VECTOR_RANGE .02 $vec_param]=>{{$yield_distance_as: range_dist}})=>[KNN 10 @embedding $knn_vec]=>{{$yield_distance_as: knn_dist}}')
        .sort_by(f'knn_dist') #asc is default order
        .return_fields("prompt:abbrev", "response", "knn_dist")
        .dialect(2)
    )
    res = vindex.search(query, query_params = {'vec_param': query_vector_as_bytes, 'knn_vec': query_vector_as_bytes})
    return res.docs

### LLM / AI Setup / functions ###
# where is the LLM library of 'weights'?
# what engine will we use to answer our prompts?
#lib_path ='/Users/owentaylor/Library/Application Support/nomic.ai/GPT4All/llama-2-7b-chat.ggmlv3.q4_0.bin'
lib_path = '/Users/owentaylor/Library/Application Support/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin'

def create_and_fetchLLM():
    # create LLM object:
    return(GPT4All(model=lib_path,verbose=False,repeat_penalty=1.5))

# a little prompt engineering is needed to get the answers in a usable format:

template="""The prompt that follows is a question you must answer:
    
    Question: the input you must answer {question}

    Format the answer as a brief article
      
    Begin! ...
    """    

# try your hand at prompt engineering by playing with this template: (rename the variable below to 'template')
template_llama_rename_me_to_template_if_needed="""
    The prompt below is a question to answer.
    You are a gangster from 1940
    If you don't know the answer, celebrate that you don't know and congratulate the user, don't try to make up an answer.
    Use the following format:

    Question: the input question you must answer {question}

    Begin! Remember to speak as an educator when giving your answer and do not use emojis. 
    Do not add prefixes like Human: and AI:. 
    Do not prefix your answer with any caveat.
    
    Keep the Answer to under 150 words.
    If asked to write a poem, make that the priority and ensure every line in your response uses iambic pentameter.

    Answer: Step through this with me ...
    """    

#display something to UI (Web Page):
st.title('Is This Thing On?')
# get user input/prompt/question:
user_input=st.text_input('What is your question? (prompt)')

## useful examples:
# https://blog.baeke.info/2023/03/21/storing-and-querying-for-embeddings-with-redis/ 
# https://github.com/RediSearch/RediSearch/blob/master/docs/docs/vecsim-range_queries_examples.ipynb
sentences = [user_input]
model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
# create the numpy array encoding representation of the sentences/prompt:
embeddings = model.encode(sentences)
print(f'\nHere is the type of the embedding object: {type(embeddings)}')
# convert the embedding numpy array to bytes for use in Redis Hash:
embedding_vector_as_bytes = embeddings.tobytes()

SCHEMA = [
    VectorField("embedding", "FLAT", {"TYPE": "FLOAT32", "DIM": 768, "DISTANCE_METRIC": "COSINE"}),
]

# Create the index
try:
    redis_connection.ft(index_name).create_index(fields=SCHEMA, definition=IndexDefinition(prefix=["prompt:"], index_type=IndexType.HASH))
except Exception as e:
    print("Index already exists")
    #print(conn.ft(index_name).info())


# pass the user text/prompt to the LLM Chain and print out the response: 
# also, check for a suitable cached response
if user_input:
    start_time=time.perf_counter()
    trimmed_question = compact_string_for_keyname(sentences)
    prompt_hash = {
        "prompt:abbrev": trimmed_question,
        "embedding": embedding_vector_as_bytes,
        "response": ""
    }

    if not redis_connection.exists(f"prompt:{trimmed_question}"):
        redis_connection.hset(name=f"prompt:{trimmed_question}", mapping=prompt_hash)

    results = vec_search(redis_connection.ft(index_name),embedding_vector_as_bytes)
    #print(results)
    # only write the response to an empty prompt: hashkey
    # due to the sortby this should always be the one at zero index
    # we expect at most 10 docs (KNN 10 is specified in query)
    # they are in asc distance order - smaller the distance the better
    found_useful_result = False
    llm_response = ""
    for next_result in results:
        if not next_result.response == "":
            # ensure that we only reuse a response that is suitable
            # cached responses are stored as strings in redis 
            # and the keyname for the relevant string is stored in the 
            # response attribute of the prompt Hash object in Redis.
            # This decouples the storage of the answer allowing for more reuse
            # answers stored this way can easily be edited and all similar prompts will 
            # point to this now, updated response|answer
            if (float(next_result.knn_dist) < .2) and (next_result.response != "") :
                print(f'\nFound a match! -->\n {next_result}\n')
                found_useful_result = True
                # llm_reponse_cache_key should point to a string in redis:
                llm_response_cache_key=next_result.response
                print(f'keyname of cached response: {llm_response_cache_key}')
                llm_response=redis_connection.get(llm_response_cache_key)
        if found_useful_result: 
            # write the nearby result as the answer to this object in redis
            # in this way, multiple prompts will share the same result
            redis_connection.hset(results[0].id,'response',llm_response_cache_key)
            break
    # we have exhausted all the potential matches:
    if not found_useful_result:
        print('\n No suitable response has been cached.  Generating new Response...\n')
        # create a new AI-generated result as the answer            
        ## Setup the LLM PromptTemplate so it can be added to the chain:
        prompt_template=PromptTemplate(template=template,input_variables=['question'])

        # bring prompt and LLM chain together:
        llmChain = LLMChain(prompt=prompt_template,llm=create_and_fetchLLM())

        llm_response = llmChain.run(user_input)
        x = redis_connection.incr('prompt:responsekeycounter')
        # store the full response in a string in redis under the keyname prompt:response:x
        redis_connection.set(f'prompt:response:{x}',llm_response)
        # write the cached response keyname to the response attribute in redis:
        # due to sorting of the results by KNN distance ASC the first result should be our target:
        redis_connection.hset(results[0].id,'response',(f'prompt:response:{x}'))            
    
    # output whatever the result it to the User Interface:
    st.write(llm_response.replace('\n', '<br />'),unsafe_allow_html=True)
    print(f'\n\tElapsed Time to respond to user prompt was: {(time.perf_counter()-start_time)*1} seconds\n')
